{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TutorialOnGradientCentralization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNE3ZMBYQGMYWJj1KdqUFxn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCSchmidt/My-Projects/blob/main/TutorialOnGradientCentralization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU_QSmAZz7wx"
      },
      "source": [
        "## Use of Gradient Centralization for Better Training Performance\n",
        "\n",
        "This is a tutorial on the new optimization technique, [Gradient Centralization](https://arxiv.org/abs/2004.01461), for deep neural networks developed by Hongwei Yong, et all that operates directly on gradients by centralizing the gradient vectors to have mean zero. Viewed as a projected gradient descent method with a constrained loss function, gradient centralization can regularize both the weight space and output feature space to boost the generalization performance of deep neural networks. Gradient centralization improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. \n",
        "\n",
        "If `tensorflow_datasets` is needed it can be installed using the\n",
        "```\n",
        "pip install tensorflow-datasets\n",
        "```\n",
        "command.  This tutorial builds a Gradient Centralization example but there is a package to speed up the process available at [gradient-centralization-tf](https://github.com/Rishit-dagli/Gradient-Centralization-TensorFlow)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR771eK75ZFR",
        "outputId": "eb90b631-daba-4d6f-b742-c098cc32b6ee"
      },
      "source": [
        "pip install tensorflow-datasets"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.12.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (21.2.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.1.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets) (57.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-XnXUQp1_QU"
      },
      "source": [
        "## The Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTEeltqGzWXD"
      },
      "source": [
        "from time import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6LCxEvj2qn9"
      },
      "source": [
        "## Prepare the Data\n",
        "\n",
        "For this example, we use the [Horses or Humans\n",
        "dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans) put together by Laurence Moroney."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXDo4B602u3P",
        "outputId": "46c5e70b-e933-4e22-d194-43af57930455"
      },
      "source": [
        "num_classes = 2\n",
        "input_shape = (300, 300, 3)\n",
        "dataset_name = \"horses_or_humans\"\n",
        "batch_size = 128\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "(train_ds, test_ds), metadata = tfds.load(\n",
        "    name=dataset_name,\n",
        "    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "print(f\"Image shape: {metadata.features['image'].shape}\")\n",
        "print(f\"Training images: {metadata.splits['train'].num_examples}\")\n",
        "print(f\"Test images: {metadata.splits['test'].num_examples}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image shape: (300, 300, 3)\n",
            "Training images: 1027\n",
            "Test images: 256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45bVeYlt6FCX"
      },
      "source": [
        "## Use Data Augmentation\n",
        "\n",
        "We rescale the data to `[0, 1]` and perform simple augmentations to our data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g4ECLJD6TJA"
      },
      "source": [
        "rescale = layers.experimental.preprocessing.Rescaling(1.0 / 255)\n",
        "\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.3),\n",
        "        layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "    # Rescale dataset\n",
        "    ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1024)\n",
        "\n",
        "    # Batch dataset\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    # Use data augmentation only on the training set\n",
        "    if augment:\n",
        "        ds = ds.map(\n",
        "            lambda x, y: (data_augmentation(x, training=True), y),\n",
        "            num_parallel_calls=AUTOTUNE,\n",
        "        )\n",
        "\n",
        "    # Use buffered prefecting\n",
        "    return ds.prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QARwF9cj8SRC"
      },
      "source": [
        "Rescale and augment the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1avZCW166UB6"
      },
      "source": [
        "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
        "test_ds = prepare(test_ds)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WRm8p1o9jk_"
      },
      "source": [
        "## Define a Model\n",
        "\n",
        "This next section defines a convolutional neural network with 16 layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwg531GY6Tz5"
      },
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(512, activation=\"relu\"),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zwbowN94f2"
      },
      "source": [
        "## Implement Gradient Centralization\n",
        "\n",
        "We will now subclass the `RMSprop` optimizer class modifying the `tf.keras.optimizers.Optimizer.get_gradients()` method where we can implement Gradient Centralization. On a high level the idea is that we obtain the gradients through back  propagation for a Dense or Convolution layer then compute the mean of the column vectors of the weight matrix and then remove the mean from each column vector. \n",
        "\n",
        "The experiments in this [this paper](https://arxiv.org/abs/2004.01461) on various\n",
        "applications, including general image classification, fine-grained image classification,\n",
        "detection and segmentation and Person ReID demonstrate that GC can consistently improve\n",
        "the performance of DNN learning.\n",
        "\n",
        "To keep things at a less complex level, this tutorial does not implement gradient clippling functionality, however this is quite easy to do.\n",
        "\n",
        "We are just creating a subclass for the `RMSprop` optimizer but this can easily be reproduced for any other optimizer or on a custom optimizer in the same way. We will be using this class in the later section when we train a model with Gradient Centralization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRuDCShjAn1L"
      },
      "source": [
        "class GCRMSprop(RMSprop):\n",
        "  def get_gradients(self, loss, params):\n",
        "    # Only need to provide a modfified get_gradients() function since we are\n",
        "    # trying to only compute the centralized gradients.\n",
        "\n",
        "    grads = []\n",
        "    gradients = super().get_gradients()\n",
        "    for grad in gradients:\n",
        "      grad_len = len(grad.shape)\n",
        "      if grad_len > 1:\n",
        "        axis = list(range(grad_len -1))\n",
        "        grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
        "      grads.append(grad)\n",
        "\n",
        "    return grads  \n",
        "\n",
        "optimizer = GCRMSprop(learning_rate=1e-4)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQGdKf8ACdT7"
      },
      "source": [
        "## Training Utilities\n",
        "\n",
        "We also create a callback which allows us to measure the total training time and the time taken for each epoch so that we have the tools to measure the impact of implementing Gradient Centralization on the model built above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUPxmnHbE66F"
      },
      "source": [
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.times = []\n",
        "  \n",
        "  def on_epoch_begin(self, batch, logs={}):\n",
        "    self.epoch_time_start = time()\n",
        "\n",
        "  def on_epoch_end(self, batch, logs={}):\n",
        "    self.times.append(time() - self.epoch_time_start)\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KplCevyFnJy"
      },
      "source": [
        "## Train the Model without Gradient Centralization\n",
        "\n",
        "First, we train the model without Gradient Centralization which we can then compare to the model trained with Gradient Centralization.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZnrJKYGF6PX",
        "outputId": "e9d6310f-1e50-4e6f-cf0e-7789f0e0388d"
      },
      "source": [
        "time_callback_no_gc = TimeHistory()\n",
        "\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=RMSprop(learning_rate=1e-4),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 147, 147, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 71, 71, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8RTI55EGV7E"
      },
      "source": [
        "We need to save the history so that we can compare the models with and without Gradient Centralization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JJ0DUzqGgGM",
        "outputId": "fbc271d1-8ce5-4358-9d6e-4444f36313d3"
      },
      "source": [
        "history_no_gc = model.fit(\n",
        "    train_ds, epochs=10, verbose=1, callbacks=[time_callback_no_gc]\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.6902 - accuracy: 0.5346\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.6630 - accuracy: 0.5959\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.6322 - accuracy: 0.6933\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.6040 - accuracy: 0.7157\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.5744 - accuracy: 0.7059\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.5338 - accuracy: 0.7400\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.5225 - accuracy: 0.7429\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4967 - accuracy: 0.7858\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4760 - accuracy: 0.7936\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4719 - accuracy: 0.7751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDAe-Sf8IE1p"
      },
      "source": [
        "## Train the Model with Gradient Centralization\n",
        "\n",
        "Next, we train the same model with Gradient Centralization. Our optimizer is the one that uses gradient centralization this time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8nDgMdOIZj2",
        "outputId": "3fb617a2-3e54-4092-f033-96cd44aac720"
      },
      "source": [
        "time_callback_gc = TimeHistory()\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy']) # optimizer = GCRMSprop(learning_rate=1e-4)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history_gc = model.fit(train_ds, epochs=10, verbose=1, callbacks=[time_callback_gc])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 147, 147, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 71, 71, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.5099 - accuracy: 0.7790\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4626 - accuracy: 0.8033\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4182 - accuracy: 0.8393\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4319 - accuracy: 0.8228\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.3920 - accuracy: 0.8608\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.4053 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.3940 - accuracy: 0.8423\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.3556 - accuracy: 0.8744\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - 17s 1s/step - loss: 0.3632 - accuracy: 0.8627\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - 16s 1s/step - loss: 0.3394 - accuracy: 0.8647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5BjqdoCJTfY"
      },
      "source": [
        "##  Comparing Performance\n",
        "\n",
        "We can now compare the performance of the two approaches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIsFHB_uJaZT",
        "outputId": "fcbf4f75-aa9c-4287-d9da-ea3a3d9b1eeb"
      },
      "source": [
        "  print(\"Not using Gradient Centralization\")\n",
        "  print(f\"Loss: {history_no_gc.history['loss'][-1]}\")\n",
        "  print(f\"Accuracy: {history_no_gc.history['accuracy'][-1]}\")\n",
        "  print(f\"Training Time: {sum(time_callback_no_gc.times)}\")\n",
        "\n",
        "  print(\"Using Gradient Centralization\")\n",
        "  print(f\"Loss: {history_gc.history['loss'][-1]}\")\n",
        "  print(f\"Accuracy: {history_gc.history['accuracy'][-1]}\")\n",
        "  print(f\"Training Time: {sum(time_callback_gc.times)}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not using Gradient Centralization\n",
            "Loss: 0.4718668758869171\n",
            "Accuracy: 0.7750730514526367\n",
            "Training Time: 165.5166256427765\n",
            "Using Gradient Centralization\n",
            "Loss: 0.3393610119819641\n",
            "Accuracy: 0.8646543622016907\n",
            "Training Time: 165.13558745384216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chg4TmmeJrVH"
      },
      "source": [
        ""
      ]
    }
  ]
}